{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPb2KOfjEm3aV4u5UWx9Zk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaiikHaider/PricePredictionML/blob/main/ProductPricePrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKI8bL4_Ti7m",
        "outputId": "81f16830-5ac6-4319-8ac9-b60126b6738c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Train shape: (75000, 4)\n",
            "Test shape: (75000, 3)\n",
            "Downloading train images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Images:  53%|█████▎    | 35308/67184 [03:04<02:24, 221.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Not able to download - https://m.media-amazon.com/images/I/51mjZYDYjyL.jpg\n",
            "HTTP Error 404: Not Found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading Images:  53%|█████▎    | 35366/67184 [03:04<02:06, 252.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Images: 100%|██████████| 67184/67184 [05:39<00:00, 198.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading test images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Images:  57%|█████▋    | 41026/72222 [03:31<01:59, 260.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Not able to download - https://m.media-amazon.com/images/I/813CjSgHj0S.jpg\n",
            "HTTP Error 404: Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Images: 100%|██████████| 72222/72222 [06:34<00:00, 183.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features for train_img_features.npy:  21%|██▏       | 14808/69476 [37:58<2:41:48,  5.63it/s]"
          ]
        }
      ],
      "source": [
        "# ====================================================\n",
        "# SMART PRODUCT PRICING CHALLENGE - PIPELINE WITH IMAGE FEATURE CACHING\n",
        "# ====================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, zipfile, urllib, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "import multiprocessing\n",
        "import random\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "\n",
        "# ----------------------------\n",
        "# PATHS\n",
        "# ----------------------------\n",
        "DATASET_FOLDER = '/content/drive/MyDrive/student_resource/dataset'\n",
        "TRAIN_CSV = os.path.join(DATASET_FOLDER, 'train.csv')\n",
        "TEST_CSV  = os.path.join(DATASET_FOLDER, 'test.csv')\n",
        "IMAGE_FOLDER = 'downloaded_product_images'\n",
        "os.makedirs(IMAGE_FOLDER, exist_ok=True)\n",
        "os.makedirs(\"submission_files\", exist_ok=True)\n",
        "\n",
        "# Cache files for image features\n",
        "TRAIN_IMG_FEATURES_FILE = 'train_img_features.npy'\n",
        "TEST_IMG_FEATURES_FILE  = 'test_img_features.npy'\n",
        "\n",
        "# ----------------------------\n",
        "# LOAD DATA\n",
        "# ----------------------------\n",
        "train = pd.read_csv(TRAIN_CSV)\n",
        "test  = pd.read_csv(TEST_CSV)\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "\n",
        "# ----------------------------\n",
        "# OUTLIER REMOVAL\n",
        "# ----------------------------\n",
        "Q1 = train['price'].quantile(0.25)\n",
        "Q3 = train['price'].quantile(0.75)\n",
        "IQR = Q3-Q1\n",
        "upper_bound = Q3 + 1.5*IQR\n",
        "train = train[train['price'] <= upper_bound]\n",
        "\n",
        "# ----------------------------\n",
        "# TEXT CLEANING\n",
        "# ----------------------------\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = ''.join(c if c.isalnum() or c.isspace() else ' ' for c in text)\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "train['clean_text'] = train['catalog_content'].apply(clean_text)\n",
        "test['clean_text']  = test['catalog_content'].apply(clean_text)\n",
        "\n",
        "# ----------------------------\n",
        "# WEIGHT/COUNT FEATURES\n",
        "# ----------------------------\n",
        "def extract_weight_features(catalog_content):\n",
        "    pattern = r\"(\\d+\\.?\\d*)\\s*(Oz|oz|LB|lb|G|g|KG|kg|Count|count|Pack|pack|Packs|packs)\"\n",
        "    match = re.search(pattern, str(catalog_content), re.IGNORECASE)\n",
        "    weight_in_grams = 0.0\n",
        "    weight_found = 0\n",
        "    count_pack = 0\n",
        "    if match:\n",
        "        try:\n",
        "            value = float(match.group(1))\n",
        "            unit = match.group(2).lower()\n",
        "            if unit=='oz': weight_in_grams=value*28.35\n",
        "            elif unit=='lb': weight_in_grams=value*453.59\n",
        "            elif unit=='kg': weight_in_grams=value*1000\n",
        "            elif unit=='g': weight_in_grams=value\n",
        "            elif unit in ['count','pack','packs']: count_pack=value\n",
        "            weight_found=1\n",
        "        except: pass\n",
        "    return pd.Series([weight_in_grams, weight_found, count_pack])\n",
        "\n",
        "train[['weight_g','weight_found','count_pack']] = train['catalog_content'].apply(extract_weight_features)\n",
        "test[['weight_g','weight_found','count_pack']] = test['catalog_content'].apply(extract_weight_features)\n",
        "\n",
        "# ----------------------------\n",
        "# DOWNLOAD IMAGES\n",
        "# ----------------------------\n",
        "def download_image(image_link, savefolder):\n",
        "    if isinstance(image_link, str):\n",
        "        filename = Path(image_link).name\n",
        "        image_save_path = os.path.join(savefolder, filename)\n",
        "        if not os.path.exists(image_save_path):\n",
        "            try:\n",
        "                urllib.request.urlretrieve(image_link, image_save_path)\n",
        "            except Exception as ex:\n",
        "                print(f'Warning: Not able to download - {image_link}\\n{ex}')\n",
        "        else:\n",
        "            return\n",
        "    return\n",
        "\n",
        "def download_images(image_links, folder):\n",
        "    download_partial = partial(download_image, savefolder=folder)\n",
        "    with multiprocessing.Pool(50) as pool:\n",
        "        for _ in tqdm(pool.imap(download_partial, image_links), total=len(image_links), desc=\"Downloading Images\"):\n",
        "            pass\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "print(\"Downloading train images...\")\n",
        "download_images(train['image_link'].dropna().unique().tolist(), IMAGE_FOLDER)\n",
        "print(\"Downloading test images...\")\n",
        "download_images(test['image_link'].dropna().unique().tolist(), IMAGE_FOLDER)\n",
        "\n",
        "# ----------------------------\n",
        "# RESNET50 IMAGE FEATURE EXTRACTION WITH CACHING\n",
        "# ----------------------------\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "TARGET_SIZE=(224,224)\n",
        "\n",
        "def extract_img_feature(file_path):\n",
        "    try:\n",
        "        img = Image.open(file_path).convert('RGB').resize(TARGET_SIZE)\n",
        "        x = np.expand_dims(np.array(img), axis=0)\n",
        "        x = preprocess_input(x)\n",
        "        feat = resnet_model.predict(x, verbose=0)\n",
        "        return feat[0]\n",
        "    except:\n",
        "        return np.zeros(2048)\n",
        "\n",
        "def bulk_extract_features(df, folder, cache_file):\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"Loading cached image features from {cache_file}\")\n",
        "        return np.load(cache_file)\n",
        "    feats=[]\n",
        "    for link in tqdm(df['image_link'].dropna(), desc=f\"Extracting features for {cache_file}\"):\n",
        "        path=os.path.join(folder, Path(link).name)\n",
        "        feats.append(extract_img_feature(path))\n",
        "    feats = np.array(feats)\n",
        "    np.save(cache_file, feats)\n",
        "    return feats\n",
        "\n",
        "train_img_feats = bulk_extract_features(train, IMAGE_FOLDER, TRAIN_IMG_FEATURES_FILE)\n",
        "test_img_feats  = bulk_extract_features(test, IMAGE_FOLDER, TEST_IMG_FEATURES_FILE)\n",
        "\n",
        "# Map back to dataframe order\n",
        "def map_features(df, feats):\n",
        "    feat_map = {link:feat for link,feat in zip(df['image_link'].dropna(), feats)}\n",
        "    out_feats = []\n",
        "    for link in df['image_link']:\n",
        "        out_feats.append(feat_map.get(link,np.zeros(2048)))\n",
        "    return np.array(out_feats)\n",
        "\n",
        "X_img_train = map_features(train, train_img_feats)\n",
        "X_img_test  = map_features(test, test_img_feats)\n",
        "\n",
        "# PCA to reduce dimensions\n",
        "pca = PCA(n_components=100)\n",
        "X_img_train = pca.fit_transform(X_img_train)\n",
        "X_img_test  = pca.transform(X_img_test)\n",
        "\n",
        "# ----------------------------\n",
        "# TEXT FEATURES\n",
        "# ----------------------------\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_text_train = tfidf.fit_transform(train['clean_text']).toarray()\n",
        "X_text_test  = tfidf.transform(test['clean_text']).toarray()\n",
        "\n",
        "# ----------------------------\n",
        "# COMBINE ALL FEATURES\n",
        "# ----------------------------\n",
        "X_train_combined = np.hstack([X_text_train, X_img_train, train[['weight_g','weight_found','count_pack']].values])\n",
        "X_test_combined  = np.hstack([X_text_test, X_img_test, test[['weight_g','weight_found','count_pack']].values])\n",
        "y_train = train['price'].values\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
        "X_test_scaled  = scaler.transform(X_test_combined)\n",
        "\n",
        "# ----------------------------\n",
        "# TRAIN/VALIDATION SPLIT\n",
        "# ----------------------------\n",
        "X_tr,X_val,y_tr,y_val = train_test_split(X_train_scaled,y_train,test_size=0.1,random_state=42)\n",
        "\n",
        "# ----------------------------\n",
        "# TRAIN XGBOOST\n",
        "# ----------------------------\n",
        "model = XGBRegressor(\n",
        "    n_estimators=500, learning_rate=0.05, max_depth=8,\n",
        "    subsample=0.8, colsample_bytree=0.8, tree_method=\"hist\", random_state=42\n",
        ")\n",
        "model.fit(X_tr, y_tr, eval_set=[(X_val,y_val)], verbose=False)\n",
        "pred_val = model.predict(X_val)\n",
        "print(\"Validation MAE:\", mean_absolute_error(y_val, pred_val))\n",
        "\n",
        "# ----------------------------\n",
        "# PREDICT TEST SET\n",
        "# ----------------------------\n",
        "test_preds = model.predict(X_test_scaled)\n",
        "test_preds = np.maximum(test_preds, 1.0)\n",
        "\n",
        "# ----------------------------\n",
        "# SAVE SUBMISSION CSV & ZIP\n",
        "# ----------------------------\n",
        "submission = pd.DataFrame({\"sample_id\": test['sample_id'], \"price\": test_preds})\n",
        "submission.to_csv(\"submission_files/test_out.csv\", index=False)\n",
        "\n",
        "with open(\"submission_files/Documentation.txt\",\"w\") as f:\n",
        "    f.write(\"\"\"Smart Product Pricing Challenge\n",
        "Model: TF-IDF + ResNet50 + XGBoost\n",
        "Features: Cleaned text, Weight/Count, PCA on image embeddings\n",
        "Evaluation: SMAPE\n",
        "Developed in Colab\n",
        "\"\"\")\n",
        "\n",
        "with zipfile.ZipFile(\"submission.zip\",\"w\") as zipf:\n",
        "    zipf.write(\"submission_files/test_out.csv\")\n",
        "    zipf.write(\"submission_files/Documentation.txt\")\n",
        "\n",
        "print(\"\\n✅ Submission ready!\")\n",
        "print(\"- submission_files/test_out.csv\")\n",
        "print(\"- submission_files/Documentation.txt\")\n",
        "print(\"- submission.zip\")\n"
      ]
    }
  ]
}